+++
title = '深度信念网络'
date = '2026-01-06T10:43:17+08:00'
author = 'RayChaux'
draft = false
tags = ["神经网络","课程笔记"]
series = ["神经网络"]
series_order = 6
math = true
+++


# 📚 神经网络方法与应用（2025）学习笔记
## 🌀 第五章 深层神经网络进阶：图神经、生成模型与变分自编码
核心目标：掌握图神经网络（GNN）、深度信念网络（DBN）、生成对抗网络（GAN）、变分自编码器（VAE）的核心原理、结构与应用，理解生成式模型与判别式模型的差异，构建从结构化到非结构化数据、从判别到生成的完整神经网络知识体系。

---

## 一、图神经网络（GNN）
### 📌 核心定位
专门处理**非结构化图数据**的神经网络，通过节点间的消息传递建模拓扑关系与属性依赖，解决节点分类、链接预测、图生成等任务，核心思想是“邻域聚合+参数共享”。

### 1.1 图的基础概念
#### 1.1.1 核心定义
参考[图论基础和表示](https://www.runoob.com/data-structures/graph-theory.html)  
- 图的表示：\(G = (V, E)\)，\(V\)为节点集合（\(|V|=N\)），\(E\)为边集合（\(|E|=M\)），节点表示实体，边表示关系。
- 邻接矩阵：无权图\(A \in \{0,1\}^{N×N}\)（\(A_{ij}=1\)表示节点相连），有权图\(W \in \mathbb{R}^{N×N}\)（\(W_{ij}\)为边权重）。
- 关键属性：节点的度（出度/入度）、邻接节点\(N(v_i)\)、k跳邻居（含自身）、路径（节点不重复的行走）、连通分量、图直径。
- 图的类型：同质图（单节点/边类型）、异质图（多类型）、二部图（节点分两类，仅跨类连边）。

#### 1.1.2 核心任务
| 任务层级       | 目标                                  | 典型应用                                  |
|----------------|---------------------------------------|---------------------------------------|
| 节点级         | 预测节点类别/属性                      | 用户标签分类、蛋白质功能预测              |
| 边级           | 预测节点间是否存在链接                  | 好友推荐、药物-蛋白质相互作用预测          |
| 子图级         | 检测社区/聚类                          | 社交圈子识别、交通路段分组                |
| 图级           | 图分类/生成/演化                      | 分子属性预测、药物发现、物理过程仿真        |

### 1.2 图表示学习
#### 1.2.1 核心痛点
图数据非结构化，邻接矩阵稀疏，需解决“拓扑信息编码”与“低维映射”，要求节点表征独立于邻居排序、包含拓扑与属性信息。

#### 1.2.2 关键方法
1. **节点嵌入（Node Embedding）**
   - 目标：将节点映射为低维稠密向量\(z_v\)，使拓扑相似节点的向量距离相近。
   - 经典方法：
     - DeepWalk：基于固定长度随机游走，最大化节点与游走邻居的相似度：
       $$\mathcal{L} = \sum_{u \in V} \sum_{v \in N_R(u)} -log \frac{exp(z_u^T z_v)}{\sum_{n \in V} exp(z_u^T z_n)}$$
       负采样优化后：$$\mathcal{L} \approx log\sigma(z_u^T z_v) - \sum_{i=1}^k log\sigma(z_u^T z_{n_i})$$
     - Node2Vec：带偏好的随机游走（平衡BFS/DFS），游走概率：
       $$\alpha = \begin{cases}\frac{1}{p} & d=0 \\ 1 & d=1 \\ \frac{1}{q} & d=2\end{cases}$$
       - \(p\)为返回参数，\(q\)为游走参数，\(d\)为节点与上一步节点的距离。

2. **子图/图嵌入**
   - 子图嵌入：对节点嵌入求和/平均，或引入虚拟节点表征子图。
   - 图嵌入：适用于小分子、蛋白质等小图，通过全局聚合节点特征得到图向量。

### 1.3 主流GNN模型
#### 1.3.1 图卷积网络（GCN）
- 核心思想：基于图的局部连接，聚合邻域节点特征与自身特征，实现层次化特征提取。
- 计算过程：
  $$h_v^k = \sigma\left( W_k \sum_{u \in N(v)} \frac{h_u^{k-1}}{|N(v)|} + B_k h_v^{k-1} \right)$$
  - 矩阵形式（拉普拉斯归一化）：
    $$H^{(k+1)} = \sigma\left( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(k)} W^{(k)} \right)$$
    - \(\tilde{A} = A + I_N\)（添加自环），\(\tilde{D}\)为\(\tilde{A}\)的度矩阵。
- 优势：参数共享，适配任意图结构；不足：深层易过平滑（节点嵌入趋同）。

#### 1.3.2 图注意力网络（GAT）
- 核心创新：引入注意力机制，自适应分配邻域节点权重，无需依赖图结构预定义。
- 注意力系数计算：
  $$\alpha_{ij} = \frac{exp\left( LeakyReLU\left( \vec{a}^T [W \vec{h}_i \| W \vec{h}_j] \right) \right)}{\sum_{k \in N(i)} exp\left( LeakyReLU\left( \vec{a}^T [W \vec{h}_i \| W \vec{h}_k] \right) \right)}$$
- 多头注意力融合：
  $$\vec{h}_i' = Concat\left( \sigma\left( \sum_{j \in N(i)} \alpha_{ij}^k W^k \vec{h}_j \right) \right) W^O$$
- 优势：自适应捕捉重要邻域，鲁棒性强；不足：计算复杂度高于GCN。

#### 1.3.3 其他模型
- GraphSAGE：对邻居采样后聚合（均值/最大值/LSTM），支持归纳学习（未见过的节点）。
- GGNN（门控GNN）：引入GRU/LSTM缓解梯度消失，支持20层以上深层网络。
- Cluster-GCN：将大图聚类为子图，降低训练复杂度，适配大规模图。

### 1.4 训练与优化
- 无监督训练：基于图结构（随机游走、重构邻接矩阵）。
- 监督训练：节点分类用交叉熵损失，链接预测用二分类损失。
- 过平滑解决方案：残差连接（Skipped-GCN）、图稀疏化/稠密化、控制网络深度。

---

## 二、深度信念网络（DBN）
### 📌 核心定位
基于玻尔兹曼机（BM）堆叠的生成式深度学习模型，通过“逐层预训练+微调”解决深层网络梯度消失问题，核心思想是“分而治之+概率建模”。

### 2.1 基础组件：玻尔兹曼机与受限玻尔兹曼机
#### 2.1.1 玻尔兹曼机（BM）
- 结构：二值随机神经元，全连接、连接对称，无层间区分。
- 能量函数与概率分布：
  $$E(x) = -\left( \sum_{i<j} w_{ij} x_i x_j + \sum_i b_i x_i \right)$$
  $$p(x) = \frac{1}{Z} exp\left( -\frac{E(x)}{T} \right)$$
  - \(Z\)为配分函数，\(T\)为温度，\(x_i \in \{0,1\}\)为神经元状态。
- 核心问题：配分函数计算复杂（\(O(2^N)\)），需吉布斯采样近似。
- 全条件概率：
  $$p(x_i=1 | x_{\backslash i}) = \sigma\left( \frac{1}{T} \sum_j w_{ij} x_j + b_i \right)$$

#### 2.1.2 受限玻尔兹曼机（RBM）
- 结构：二分图（可见层\(v\) + 隐层\(h\)），同层无连接，异层全连接。
- 能量函数与联合概率：
  $$E(v, h) = -\left( v^T W h + a^T v + b^T h \right)$$
  $$p(v, h) = \frac{1}{Z} exp(-E(v, h))$$
- 全条件概率（同层独立）：
  $$p(v_i=1 | h) = \sigma\left( a_i + \sum_j w_{ij} h_j \right)$$
  $$p(h_j=1 | v) = \sigma\left( b_j + \sum_i w_{ij} v_i \right)$$
- 训练算法：对比散度（CD-k），无需采样至热平衡，参数更新：
  $$\Delta W = \hat{v}^{(n)} h^T - v' h'^T$$
  $$\Delta a = \hat{v}^{(n)} - v', \quad \Delta b = h - h'$$

### 2.2 DBN结构与训练
#### 2.2.1 结构
- 堆叠RBM：底层为可见层，上层为隐层，顶部两层为无向RBM，其余层为有向连接。
- 联合概率分布：
  $$p(v, h^{(1)},...,h^{(L)}) = p(v | h^{(1)}) \prod_{l=1}^{L-2} p(h^{(l)} | h^{(l+1)}) p(h^{(L-1)}, h^{(L)})$$

#### 2.2.2 训练流程
1. 逐层预训练：从下到上，每层RBM独立训练，用CD-k优化参数。
2. 微调：
   - 生成式模型：Wake-Sleep算法，交替优化认知权重（上行）和生成权重（下行）。
   - 判别式模型：BP算法微调，适配分类等任务。

### 2.3 应用与优势
- 优势：无监督预训练初始化参数，缓解梯度消失；可作为生成模型或特征提取器。
- 应用：数据降维、图像重建、语音识别、推荐系统。

---

## 三、生成对抗网络（GAN）
### 📌 核心定位
2014年提出的生成式模型，基于“零和博弈”思想，通过生成器与判别器的对抗训练逼近真实数据分布，核心是“极小极大优化”。

### 3.1 核心思想与结构
#### 3.1.1 双网络架构
- 生成器（G）：输入随机向量\(z \sim p(z)\)，输出生成样本\(G(z)\)，目标是欺骗判别器。
- 判别器（D）：输入真实样本\(x \sim p_r(x)\)或生成样本\(G(z)\)，输出分类概率\(D(x)\)（1为真实，0为生成），目标是准确区分来源。

#### 3.1.2 目标函数
- 极小极大博弈：
  $$min_G max_D \mathcal{L}(D, G) = E_{x \sim p_r(x)}[log D(x)] + E_{z \sim p(z)}[log(1 - D(G(z)))]$$
- 最优判别器：
  $$D^*(x) = \frac{p_r(x)}{p_r(x) + p_\theta(x)}$$

### 3.2 训练与改进
#### 3.2.1 训练技巧
- 交替训练：判别器更新K次，生成器更新1次，平衡两者能力。
- 生成器目标替换：将\(min_G log(1 - D(G(z)))\)替换为\(max_G log D(G(z))\)，缓解梯度消失。

#### 3.2.2 经典改进模型
1. DCGAN（深度卷积GAN）
   - 结构优化：生成器用转置卷积，判别器用步长卷积（无池化）；批量归一化；生成器用ReLU（输出层Tanh），判别器用LeakyReLU。
   - 优势：生成图像更逼真，训练更稳定。

2. W-GAN
   - 核心改进：用Wasserstein距离替代JS散度，解决分布不重叠时梯度消失问题：
     $$W^1(p_r, p_\theta) = inf_{\gamma \sim \Gamma(p_r, p_\theta)} E_{(x,y) \sim \gamma}[\|x-y\|]$$
   - 目标函数：
     $$max_f E_{x \sim p_r}[f(x)] - E_{x \sim p_\theta}[f(x)]$$
     $$min_G -E_{z \sim p(z)}[f(G(z))]$$
   - 优势：训练稳定，缓解模型坍塌。

### 3.3 典型应用
- 生成任务：图像生成（二次元、人脸）、音乐生成、3D模型生成。
- 转换任务：图像风格迁移（CycleGAN）、图像修复、超分辨率重建。
- 合成任务：文本→图像、语义图→真实场景、侧脸→全脸。

---

## 四、变分自编码器（VAE）
### 📌 核心定位
结合自编码器与概率生成模型，将输入编码为低维概率分布（而非确定向量），通过采样生成新样本，核心是“变分推断+再参数化”。

### 4.1 基础铺垫：自编码器（AE）
- 结构：编码器\(z = e(x)\)（降维）+ 解码器\(\hat{x} = d(z)\)（重构）。
- 损失：重构损失\(\mathcal{L} = \|x - \hat{x}\|^2\)。
- 局限：编码空间无结构，随机采样生成无意义内容；易过拟合。

### 4.2 VAE核心原理
#### 4.2.1 概率建模
- 生成过程：\(z \sim p(z)\)（先验分布，通常为\(\mathcal{N}(0,I)\)）→ \(x \sim p(x | z)\)（似然分布）。
- 变分推断：引入近似后验\(q(z | x)\)（编码器建模），最大化证据下界（ELBO）：
  $$ELBO(q, x; \theta, \phi) = E_{z \sim q(z|x;\phi)}[log p(x|z;\theta)] - KL(q(z|x;\phi) \| p(z))$$

#### 4.2.2 再参数化技巧
- 问题：采样过程\(z \sim q(z|x)\)不可导，梯度无法回传。
- 解决方案：\(z = \mu_I + \sigma_I \odot \epsilon\)，其中\(\epsilon \sim \mathcal{N}(0,I)\)，\(\mu_I, \sigma_I\)为编码器输出。
- 优势：将采样转化为确定性变换，梯度可通过\(\mu_I, \sigma_I\)回传。

### 4.3 训练与拓展
#### 4.3.1 训练目标
- 整体损失（基于批量采样）：
  $$\mathcal{J}(\phi, \theta) = \sum_{n=1}^N \left( \frac{1}{M} \sum_{m=1}^M log p(x^{(n)}|z^{(n,m)};\theta) - KL(q(z|x^{(n)};\phi) \| \mathcal{N}(0,I)) \right)$$

#### 4.3.2 条件变分自编码器（CVAE）
- 核心改进：编码器输入\((x, y)\)，解码器输入\((z, y)\)，引入条件信息控制生成类型：
  $$L_{CVAE}(\theta, \phi) = -E_{z \sim q_{\phi}(z|x,y)} log p_\theta(x|z,y) + KL(q_{\phi}(z|x,y) \| p_\theta(z|y))$$
- 应用：生成指定类别的图像、文本。

### 4.4 优势与应用
- 优势：编码空间结构化（相近分布生成相似内容），生成过程可控；训练稳定。
- 应用：图像生成、文本生成、异常检测、药物分子设计。

---

## 五、四大生成模型对比
| 模型          | 核心思想                          | 核心结构                          | 训练关键                          | 典型应用                          | 核心优势                          |
|---------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|
| DBN           | 逐层预训练+概率建模                  | 堆叠RBM，顶部无向连接                | CD-k预训练+Wake-Sleep/BP微调          | 特征提取、数据降维                  | 缓解梯度消失，适配无监督数据          |
| GAN           | 对抗训练（零和博弈）                  | 生成器+判别器双网络                  | 交替训练+目标替换+Wasserstein距离      | 高逼真生成、图像转换                | 生成质量高，突破似然估计限制          |
| VAE           | 变分推断+概率编码                  | 推断网络+生成网络+再参数化            | ELBO优化+高斯分布假设                | 可控生成、异常检测                  | 编码空间结构化，训练稳定              |
| GNN（生成式） | 消息传递+邻域聚合                  | 图卷积/注意力层+生成层                | 节点嵌入预训练+图结构重构            | 分子生成、物理仿真                  | 适配非结构化图数据，建模拓扑依赖        |

---

## 📋 核心总结
1. **GNN核心**：通过消息传递建模图拓扑，GCN适配一般图，GAT自适应邻域权重，Cluster-GCN适配大规模图，过平滑是深层GNN的主要挑战。
2. **生成模型对比**：
   - DBN：基于玻尔兹曼机堆叠，适合无监督特征提取与降维。
   - GAN：对抗训练生成质量高，但需平衡双网络能力，W-GAN解决稳定性问题。
   - VAE：概率编码+再参数化，生成可控且稳定，适合结构化生成任务。
3. **关键技巧**：
   - GNN：残差连接缓解过平滑，注意力机制提升自适应能力。
   - 生成模型：预训练（DBN）、交替训练（GAN）、变分推断（VAE）是各自核心训练逻辑。
4. **应用选型**：
   - 图数据生成/分析：GNN；高逼真图像生成：GAN；可控生成/异常检测：VAE；无监督特征提取：DBN。

要不要我帮你整理一份**四大模型核心公式速查表**，或生成一份**生成模型实战选型指南**（含任务匹配、参数设置、常见问题解决方案）？