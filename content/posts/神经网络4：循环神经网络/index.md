+++
date = '2025-12-05T17:02:54+08:00'
draft = false
title = '循环神经网络'
author = 'RayChaux'
tags = ["课程笔记", "神经网络"]
series = ["神经网络"]
series_order = 4
math = true
+++

# 📚 神经网络方法与应用（2025）学习笔记
## 🌀 第四章 深层神经网络：循环神经网络（RNN）与注意力机制
核心目标：掌握循环神经网络的基础模型、训练算法、核心问题与改进方案（LSTM/GRU），理解注意力机制的原理及在RNN中的应用，明确RNN与CNN的适用场景差异，构建时序数据建模的完整知识体系。

---

## 一、RNN基础模型
### 📌 核心定位
RNN是一类专门处理**时序数据**的深层神经网络，通过循环连接保留历史信息，解决序列的上下文依赖问题，核心思想是“状态编码+参数共享”。

### 1.1 设计思想
#### 1.1.1 核心痛点
传统MLP无法处理可变长度序列，且n-阶语言模型存在“依赖受限”和“参数爆炸”问题，RNN通过两大设计思想解决：

1. **状态编码（短期记忆）**  
   - 核心：将序列前序信息编码为**隐含状态** \(h_t\)，当前输出依赖于当前输入 \(x_t\) 和前一状态 \(h_{t-1}\)（马尔可夫假设）。  
   - 示例：计算 \(12+3+5+10+23+6\) 时，人类会保留中间结果，RNN的隐含状态类似中间结果存储。

2. **参数共享（时间平稳性）**  
   - 核心：同一网络在不同时间步共享相同参数（\(U、W、V\)），即“一个特征在某个时刻有效，在其他时刻也有效”。  
   - 优势：参数数量与序列长度无关，避免参数爆炸，适配任意长度序列。

#### 1.1.2 核心概念
- **语言模型**：建模 \(P(word_i | word_1,...,word_{i-1})\)，预测序列中下一个词的概率分布。  
- **词向量表示**：替代one-hot编码（维度灾难），常用300/500/1000维稠密向量（如Word2Vec），实现“表示学习”。  
- **特殊符号**：[SOS]（序列开始）、[EOS]（序列结束）。

### 1.2 循环连接与循环层
#### 1.2.1 网络结构
- 总体结构：输入层 \(x_t\) → 循环层（隐含状态 \(h_t\)）→ 输出层 \(y_t\)，循环层通过自身反馈实现记忆传递。  
- 循环层计算（核心公式）：  
  $$h_t = tanh(W h_{t-1} + U x_t + b)$$  
  $$y_t = V h_t$$  
  - \(W\)：隐含状态转移权重，\(U\)：输入到隐含层权重，\(V\)：隐含层到输出权重，\(b\)：偏置。  
  - 记忆体现：\(h_t\) 递归依赖 \(h_{t-1}\)，最终包含所有前序输入信息：  
    $$h_t = tanh(W \cdot tanh(W h_{t-2} + U x_{t-1} + b) + U x_t + b)$$

#### 1.2.2 通用近似定理
- 若RNN有足够多sigmoid型隐藏神经元，可任意精度逼近非线性动力系统：  
  $$s_t = g(s_{t-1}, x_t), \quad y_t = o(s_t)$$  
  - \(s_t\) 为隐状态，\(g\) 为状态转换函数，\(o\) 为输出函数。

### 1.3 RNN拓展结构
#### 1.3.1 双向循环神经网络（Bi-RNN）
- 核心：同时利用“前向序列”和“后向序列”信息，解决上下文双向依赖问题（如词性标注、机器翻译）。  
- 结构：前向隐含层 \(h_t\)（从左到右）+ 后向隐含层 \(g_t\)（从右到左），输出融合两者信息：  
  - 拼接（Concat）：\(s_t = [h_t, g_t]\)（兼容性更强，常用）  
  - 求和（Add）：\(s_t = h_t + g_t\)  

#### 1.3.2 堆叠循环神经网络（Deep RNN）
- 核心：多层循环层堆叠，解决“空间深度不足”问题，增强特征提取能力。  
- 计算过程：  
  $$h_t^1 = tanh(W_1 h_{t-1}^1 + U_1 x_t)$$  
  $$h_t^l = tanh(W_l h_{t-1}^l + U_l h_t^{l-1})$$  
  $$\hat{y}_t = softmax(V h_t^l)$$  
  - \(l\) 为层数，上层输入为下层隐含状态。

> 💡 关键结论：双向RNN解决“上下文同时依赖”，堆叠RNN解决“特征提取深度”，两者可组合使用（如Deep Bi-RNN）。

---

## 二、RNN架构与训练
### 📌 核心逻辑：前向传播+反向传播（时间维度）+ 参数更新
### 2.1 应用模式
RNN根据输入输出关系，分为4种核心应用模式，覆盖时序任务全场景：

| 模式类型         | 核心逻辑                                  | 典型应用                                  | 结构特点                                  |
|------------------|-------------------------------------------|-------------------------------------------|-------------------------------------------|
| 多对一（Many-to-One） | 输入序列→单个输出                          | 情感分类、文档分类、雷达干扰感知          | 取最后时刻隐含状态或加权平均              |
| 一对多（One-to-Many） | 单个输入→输出序列                          | 图像描述、文本生成、乐谱生成              | 输入初始化隐含状态，后续时刻自回归生成      |
| 同步多对多（Synchronous Many-to-Many） | 输入序列→同步输出序列（等长）              | 实时故障监测、目标跟踪、词性标注          | 每个时刻均有输入和输出，时序对齐          |
| 异步多对多（Asynchronous Many-to-Many） | 输入序列→输出序列（不等长）                | 机器翻译、语音转文字                      | 编码器（输入序列）+ 解码器（输出序列）结构  |

> ❓ 考点：机器翻译采用“异步多对多”模式，而非同步模式；图像描述是“一对多”模式的典型应用。

### 2.2 训练算法
#### 2.2.1 随时间反向传播（BPTT）
- 核心：将RNN沿时间维度展开为深度前馈网络，按时间反向传播梯度，更新共享参数。  
- 关键步骤：
  1. 前向计算：按时间步计算 \(h_t\) 和 \(y_t\)，存储中间结果。  
  2. 损失函数：序列总损失 \(L = \sum_{t=1}^T L_t\)（\(L_t\) 为t时刻损失）。  
  3. 梯度计算：  
     - 误差项定义：\(\delta_{t,k} = \frac{\partial L_t}{\partial z_k}\)（\(z_k = W h_{k-1} + U x_k + b\)）  
     - 误差传播：\(\delta_{t,k} = diag(f'(z_k)) W^T \delta_{t,k+1}\)（沿时间反向传递）  
     - 参数梯度：  
       $$\frac{\partial L}{\partial W} = \sum_{t=1}^T \sum_{k=1}^t \delta_{t,k} s_{k-1}^T$$  
       $$\frac{\partial L}{\partial U} = \sum_{t=1}^T \sum_{k=1}^t \delta_{t,k} x_{k-1}^T$$  
       $$\frac{\partial L}{\partial V} = \sum_{t=1}^T \frac{\partial L_t}{\partial \hat{y}_t} h_t^T$$

- 缺点：长序列易导致梯度消失/爆炸（误差项连乘 \(W^T\)），需存储所有时刻中间状态，空间复杂度高。

#### 2.2.2 实时循环学习（RTRL）
- 核心：前向计算时同步计算梯度，无需存储历史状态，适合在线学习和无限长序列。  
- 关键公式（隐状态对参数的偏导数）：  
  $$\frac{\partial s_t}{\partial w_{ij}} = \left( \mathbb{I}_i([s_{t-1}]_j) + \frac{\partial s_{t-1}}{\partial w_{ij}} W^T \right) diag(f'(z_t))$$  
  $$\frac{\partial s_t}{\partial u_{ij}} = \left( \mathbb{I}_i([x_{t-1}]_j) + \frac{\partial s_{t-1}}{\partial u_{ij}} W^T \right) diag(f'(z_t))$$  
- 优点：实时更新，空间复杂度低；缺点：计算量大于BPTT，梯度估计精度略低。

#### 2.2.3 BPTT vs RTRL 对比
| 对比维度       | BPTT                                  | RTRL                                  |
|----------------|---------------------------------------|---------------------------------------|
| 梯度计算方式   | 反向模式（沿时间回溯）                  | 前向模式（同步计算）                  |
| 空间复杂度     | 高（需存储所有时刻中间状态）            | 低（仅存储当前状态梯度）              |
| 时间复杂度     | 低                                    | 高                                    |
| 适用场景       | 离线训练、有限长序列                  | 在线学习、无限长序列                  |
| 梯度精度       | 高                                    | 较高                                  |

### 2.3 训练技巧
- 随机梯度下降（SGD）：随机洗牌数据，用样本子集更新参数，易跳出局部极小值。  
- 动量项：\(\Delta W = \alpha \Delta W_{prev} - \eta \nabla L\)，缓解震荡，加速收敛。  
- 学习率衰减：指数衰减（\(\eta = \eta_0 e^{-kt}\)）或1/t衰减（\(\eta = \eta_0/(1+kt)\)），减少后期震荡。  
- 数据增强：时序数据可通过时间拉伸、噪声注入、翻转等方式扩充样本。

---

## 三、RNN的核心问题与改进方案
### 📌 核心问题：长程依赖（梯度消失/爆炸）、记忆容量有限
### 3.1 核心问题分析
#### 3.1.1 梯度消失/爆炸的本质
- 误差项传播公式：\(\delta_{t,k} = \prod_{\tau=k}^{t-1} (diag(f'(z_\tau)) W^T) \delta_{t,t}\)  
- 关键因子：\(\gamma \sigma_{max}\)（\(\gamma\) 为激活函数导数上界，\(\sigma_{max}\) 为 \(W^T\) 最大奇异值）：  
  - 若 \(\gamma \sigma_{max} < 1\)：梯度随时间步指数衰减 → 梯度消失（更常见）。  
  - 若 \(\gamma \sigma_{max} > 1\)：梯度随时间步指数增长 → 梯度爆炸。

#### 3.1.2 梯度爆炸的解决方法
- 权重衰减：L1/L2正则化限制 \(W\) 取值，降低 \(\sigma_{max}\)。  
- 梯度截断：当梯度模大于阈值时，归一化到阈值范围内（\(grad = threshold \cdot grad / \|grad\|\)）。  
- 截断BPTT（Truncated BPTT）：限制梯度回溯的时间步（如仅回溯20步），牺牲长程依赖换取训练稳定。

### 3.2 改进模型1：长短期记忆网络（LSTM）
#### 3.2.1 核心创新：门控机制+记忆单元
LSTM通过3个门控和1个记忆单元（Cell State），灵活控制信息的“遗忘、输入、输出”，解决梯度消失问题。

#### 3.2.2 门控机制与计算过程
1. **遗忘门（Forget Gate）**：控制上一时刻记忆单元 \(c_{t-1}\) 的遗忘比例：  
   $$f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)$$  
   - \(f_t \in [0,1]\)，越接近1，保留信息越多。

2. **输入门（Input Gate）**：控制当前候选状态 \(c_t'\) 的输入比例：  
   $$i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)$$  
   $$c_t' = tanh(W_c x_t + U_c h_{t-1} + b_c)$$

3. **记忆单元更新**：遗忘旧信息+输入新信息：  
   $$c_t = c_{t-1} \odot f_t + c_t' \odot i_t$$  
   - \(\odot\) 为元素-wise乘法，记忆单元直接传递，梯度不易消失。

4. **输出门（Output Gate）**：控制记忆单元向隐含状态 \(h_t\) 的输出比例：  
   $$o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)$$  
   $$h_t = tanh(c_t) \odot o_t$$

#### 3.2.3 梯度传播优势
- 记忆单元 \(c_t\) 采用恒等映射（无激活函数或梯度为1的激活），梯度可直接沿时间传递，避免指数衰减。  
- 门控机制动态调整梯度传播路径，使模型自适应学习长/短期依赖。

### 3.3 改进模型2：门控循环单元（GRU）
#### 3.3.1 核心创新：简化门控，合并记忆单元与隐含状态
GRU移除单独的记忆单元，用2个门控替代LSTM的3个门，结构更简洁，计算量更小。

#### 3.3.2 门控机制与计算过程
1. **重置门（Reset Gate）**：控制上一时刻隐含状态 \(h_{t-1}\) 对当前候选状态的影响：  
   $$r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r)$$  
   $$\hat{h}_t = tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h)$$  
   - \(r_t \in [0,1]\)，越接近0，越忽略历史信息。

2. **更新门（Update Gate）**：合并LSTM的遗忘门和输入门，控制历史状态的保留比例和新状态的输入比例：  
   $$z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z)$$  
   $$h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \hat{h}_t$$  
   - \(z_t \in [0,1]\)，越接近1，保留历史状态越多。

#### 3.3.3 LSTM vs GRU 对比
| 对比维度       | LSTM                                  | GRU                                  |
|----------------|---------------------------------------|---------------------------------------|
| 门控数量       | 3个（遗忘/输入/输出）+ 记忆单元        | 2个（重置/更新）                      |
| 结构复杂度     | 高                                    | 低                                    |
| 计算量         | 大                                    | 小                                    |
| 适用场景       | 长序列、复杂依赖（如机器翻译）          | 中等长度序列、资源受限场景（如端侧）    |
| 性能           | 略优（复杂任务）                      | 与LSTM相当（简单任务）                |

> ❓ 考点：LSTM和GRU的门控均使用sigmoid激活函数（输出[0,1]）；候选状态使用tanh激活函数（输出[-1,1]）；GRU在 \(z_t=0、r_t=1\) 时退化为简单RNN。

---

## 四、注意力机制（Attention Mechanism）
### 📌 核心思想：让模型“主动关注”输入中与当前任务最相关的部分，解决RNN的“信息瓶颈”问题
### 4.1 注意力机制基础
#### 4.1.1 核心定义
- 输入：N组信息 \(X = [x_1, x_2,...,x_N]\)（如RNN的隐含状态序列）。  
- 查询向量（Query）：与当前任务相关的向量（如解码器当前隐含状态）。  
- 注意力分布：输入信息与查询向量的相关性概率分布：  
  $$\alpha_n = softmax(s(x_n, q)) = \frac{exp(s(x_n, q))}{\sum_{j=1}^N exp(s(x_j, q))}$$  
  - \(s(x_n, q)\) 为打分函数，衡量 \(x_n\) 与 \(q\) 的相关性。

#### 4.1.2 常用打分函数
| 打分函数类型   | 表达式                                  | 特点                                  |
|----------------|---------------------------------------|---------------------------------------|
| 点积模型       | \(s(x_n, q) = x_n^T q\)                | 计算高效，要求 \(x_n\) 与 \(q\) 维度一致 |
| 缩放点积模型   | \(s(x_n, q) = \frac{x_n^T q}{\sqrt{d}}\) | 缓解高维下的方差问题（Transformer核心） |
| 加性模型       | \(s(x_n, q) = v^T tanh(W x_n + U q)\)  | 维度可不同，泛化能力强                |
| 双线性模型     | \(s(x_n, q) = x_n^T W q\)              | 泛化性最优，参数量略大                |

#### 4.1.3 注意力输出（加权平均）
$$att(X, q) = \sum_{n=1}^N \alpha_n x_n$$  
- 软性注意力：加权平均所有输入（可导，支持反向传播）。  
- 硬性注意力：选择概率最高的单个输入（不可导，需采样技巧）。

#### 4.1.4 变体形式
- 键值对注意力（Key-Value）：输入拆分为 \(K=[k_1,...,k_N]\)（键）和 \(V=[v_1,...,v_N]\)（值），注意力分布基于 \(K\)，输出基于 \(V\)：  
  $$att((K,V), q) = \sum_{n=1}^N \alpha_n v_n$$  
- 多头注意力（Multi-Head）：多个查询向量并行计算注意力，结果拼接，捕捉多维度相关性：  
  $$att((K,V), Q) = att((K,V), q_1) \oplus att((K,V), q_2) \oplus...\oplus att((K,V), q_M)$$

### 4.2 RNN中的注意力机制
#### 4.2.1 核心作用
解决RNN编码器-解码器架构的“信息瓶颈”（编码器仅用最后一个隐含状态传递信息），让解码器每个时刻都能关注编码器的所有隐含状态。

#### 4.2.2 与RNN门控的关联
- LSTM/GRU的门控机制是“隐性注意力”：通过门控权重控制信息流动，但权重仅依赖当前和前一状态。  
- 注意力机制是“显性注意力”：通过打分函数计算全局相关性，权重可解释（如翻译时关注输入对应词）。

#### 4.2.3 典型应用
- 机器翻译：解码器每个时刻关注输入句子的对应词。  
- 图像描述：编码器（CNN）提取图像特征，解码器（RNN）关注图像局部区域。  
- 阅读理解：根据问题（Query）关注文章（Key-Value）的相关段落。

> 💡 关键结论：注意力机制的数学本质是“注意力分布 × 输入信息”，核心操作是乘法；其核心价值是从“被动接收信息”到“主动选择信息”，提升模型对长序列和复杂依赖的建模能力。

---

## 五、RNN与CNN的核心对比
| 对比维度       | 循环神经网络（RNN）                      | 卷积神经网络（CNN）                      |
|----------------|-------------------------------------------|-------------------------------------------|
| 核心假设       | 时序依赖性（序列数据）                    | 局部性+不变性（空间数据）                  |
| 建模对象       | 时间维度（文本、语音、时序信号）            | 空间维度（图像、视频帧）                  |
| 核心思想       | 状态编码+参数共享（时间维度）              | 局部连接+参数共享（空间维度）              |
| 长程依赖处理   | LSTM/GRU+注意力机制                      | 深层堆叠+空洞卷积                        |
| 并行计算       | 弱（需按时间步顺序计算）                  | 强（卷积操作可并行）                      |
| 典型结构       | 编码器-解码器、双向/堆叠RNN               | 卷积层+池化层+全连接层                    |
| 核心问题       | 梯度消失/爆炸、信息瓶颈                  | 参数爆炸（全连接）、过拟合                |

---

## 📋 核心总结
1. **RNN核心**：通过循环连接和参数共享建模时序依赖，隐含状态是记忆载体，BPTT和RTRL是核心训练算法。  
2. **改进关键**：LSTM/GRU通过门控机制解决梯度消失，注意力机制解决信息瓶颈，三者共同支撑长序列建模。  
3. **应用场景**：RNN适配时序任务（预测、生成、翻译），CNN适配空间任务（分类、检测、分割），两者可融合（如CNN提取图像特征+RNN生成描述）。  
4. **关键技巧**：训练时用随机梯度下降+动量+学习率衰减；长序列优先用LSTM/GRU+注意力；资源受限场景用GRU替代LSTM。
