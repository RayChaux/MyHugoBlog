+++
date = '2025-11-29T17:02:54+08:00'
draft = false
title = '卷积神经网络'
author = 'RayChaux'
tags = ["课程笔记", "神经网络"]
series = ["神经网络"]
series_order = 3
math = true
+++


# 📚 神经网络方法与应用（2025）学习笔记
## 🌀 第四章 深层神经网络：CNN与RNN基础
核心目标：掌握卷积神经网络（CNN）的核心思想、层次结构、训练方法、发展历程及轻量化技术，理解循环神经网络（RNN）的基础定位，构建深层神经网络的知识框架。

---

## 一、卷积神经网络（CNN）基本原理
### 📌 核心思想：局部连接与参数共享
CNN源于生物视觉系统启发，通过**局部连接**和**参数共享**解决全连接网络在图像任务中的参数爆炸、空间信息丢失问题，实现层次化特征提取。

### 1.1 全连接网络的局限性
- **参数爆炸**：以256×256灰度图为例，输入维度65536，若第一个隐含层10000个神经元，参数量约6.5×10⁸（6亿+），计算负担极大。
- **空间信息丢失**：Flatten操作破坏图像像素间的局部关联关系。
- **不变性难保持**：图像目标的位置、尺度变化会影响分类效果。

### 1.2 核心概念解析
#### 1.2.1 局部连接
- 定义：下一层神经元仅与上一层局部区域的神经元连接（对应图像局部特征）。
- 优势：参数量与输入维度无关，仅与局部连接范围（卷积核大小）相关。
- 示例对比：
  | 连接方式 | 参数量（输入11个神经元，隐含层9个神经元） |
  |----------|------------------------------------------|
  | 全连接   | 9×11 + 9 = 108                           |
  | 局部连接（连接3个） | 9×3 + 9 = 36                           |

#### 1.2.2 参数共享
- 定义：同一层所有神经元的局部特征映射使用相同连接参数（卷积核）。
- 核心逻辑：同一特征（如边缘、纹理）在图像不同位置的响应一致。
- 优势：参数量进一步降低，与隐含层神经元个数无关。
- 示例对比：局部连接+参数共享后，参数量仅3+1=4（含偏置）。

#### 1.2.3 卷积运算
- 本质：局部连接+参数共享 = 卷积（忽略翻转，CNN中实际为互相关运算）。
- 数学定义：
  - 连续情况：$$(f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t-\tau) d\tau$$
  - 离散情况：$$(f * g)(n) = \sum_{m=-\infty}^{\infty} f(m) g(n-m)$$
- 图像卷积流程：
  1. 取输入图像局部块（对应局部连接）。
  2. 与卷积核做内积（线性加权求和）。
  3. 滑窗移动（对应参数共享），重复上述步骤。

#### 1.2.4 卷积核心参数与变体
##### （1）输出尺寸计算
- 基础公式（无Padding、Stride=1）：
  $$w_{new} = w - k + 1, \quad h_{new} = h - k + 1$$
  - $w,h$：输入宽高，$k$：卷积核大小。
- 含Stride和Padding：
  $$w_{new} = \left\lfloor \frac{w - k + 2 \times padding}{stride} \right\rfloor + 1$$
  $$h_{new} = \left\lfloor \frac{h - k + 2 \times padding}{stride} \right\rfloor + 1$$
- 等宽卷积Padding设置：
  $$P = \begin{cases} \frac{k-1}{2} & k=2n+1（奇数） \\ \frac{k}{2} & k=2n（偶数，可能重心偏移） \end{cases}$$

##### （2）常用卷积变体
| 变体类型       | 核心作用                                  | 关键说明                                  |
|----------------|-------------------------------------------|-------------------------------------------|
| Stride（步长） | 降低特征图尺寸，减少计算量                | Stride=2时，尺寸减半（近似）                |
| Padding（填充） | 保持输出尺寸，避免边缘特征丢失            | 常用“Same Padding”（等宽）、“Valid Padding”（无填充） |
| 空洞卷积       | 增大感受野，不增加计算量                  |  dilation=2时，3×3卷积等效5×5感受野        |
| 1×1卷积        | 调整通道数，实现特征融合                  | 参数量少（1×1×d_in×d_out），无尺寸变化      |
| 三维卷积       | 处理三维数据（如视频、点云）              | 卷积核尺寸k×k×k，保留时间/空间维度信息      |

### 1.3 核心计算量与参数量
- 参数量（单卷积核）：$$Parameters = k \times k \times d_{in}$$（$d_{in}$为输入通道数）
- 计算量（FLOPs）：$$FLOPs \approx w_{new} \times h_{new} \times k \times k \times d_{in} \times d_{out}$$（$d_{out}$为输出通道数）
- 缓存量：$$Memory = w_{new} \times h_{new} \times d_{out}$$

> 💡 关键结论：CNN的参数量和计算量仅与卷积核大小、通道数相关，与输入图像尺寸弱相关，解决了全连接网络的参数爆炸问题。

---

## 二、CNN的层次化组成
### 📌 典型结构：卷积层 + 池化层 + 全连接层
核心目标：通过层级化结构实现“低级特征→中级特征→高级特征”的递进提取。

### 2.1 卷积层
#### 2.1.1 核心功能
提取图像局部特征（边缘、纹理、形状等），不同卷积核对应不同特征。

#### 2.1.2 激活函数
- 作用：引入非线性，使网络能拟合复杂函数。
- 常用类型：
  | 函数类型       | 表达式                                  | 导数                                  | 优势                                  |
  |----------------|---------------------------------------|---------------------------------------|---------------------------------------|
  | ReLU           | $g(z) = max(0, z)$                    | $g'(z) = sgn(z)$（x≥0时为1，否则为0） | 缓解梯度消失，计算简单，稀疏激活        |
  | Logistic       | $g(z) = \frac{1}{1+e^{-z}}$            | $g'(z) = g(z)(1-g(z))$                | 输出范围[0,1]，可表示概率              |
  | 双曲正切       | $g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ | $g'(z) = 1 - g(z)^2$                  | 输出范围[-1,1]，对称分布                |

#### 2.1.3 多通道卷积
- 输入：$w \times h \times d_{in}$（$d_{in}$为输入通道数，如RGB图像d=3）。
- 卷积核：$k \times k \times d_{in} \times d_{out}$（$d_{out}$为输出通道数，即卷积核个数）。
- 计算：每个输出通道对应一个$k \times k \times d_{in}$的卷积核，与输入所有通道卷积后求和+偏置，经激活函数输出。
- 参数量示例：32×32×3输入，5×5×3卷积核×6个，参数量$(5×5×3+1)×6=456$。

### 2.2 池化层
#### 2.2.1 核心功能
- 降低特征图维度，减少计算量和过拟合风险。
- 保持特征的平移/尺度不变性。
- 非参数化操作（无训练参数）。

#### 2.2.2 常用策略
| 池化类型       | 计算方式                                  | 优势                                  | 劣势                                  |
|----------------|-------------------------------------------|---------------------------------------|---------------------------------------|
| 最大值池化     | $max(s_1, s_2, s_3, s_4)$（采样块内最大值） | 保留局部关键特征，鲁棒性强            | 丢失部分细节信息                      |
| 平均值池化     | $\frac{s_1+s_2+s_3+s_4}{4}$（采样块内均值） | 保留全局信息，平滑性好                | 特征响应较弱，易丢失关键细节            |

#### 2.2.3 其他池化策略
- 随机池化：按元素数值概率随机选择，平衡Max和Average优势。
- 混合池化：拼接/叠加Max和Average结果，兼顾细节与全局。
- 空间金字塔池化（SPP）：多尺度采样（4×4、2×2、全局），输出固定长度特征，适配任意输入尺寸。

> ❓ 思考题：Stride>1的卷积层可替代池化层吗？  
> 答：可降低维度，但池化层的非参数化、不变性保持能力是卷积层无法完全替代的，且池化层计算量更低。

### 2.3 全连接层
#### 2.3.1 核心功能
将卷积+池化提取的高维特征映射为一维向量，通过全连接计算实现分类/回归。

#### 2.3.2 输出层激活函数
- 分类任务：Softmax函数（将得分转化为概率分布）：
  $$g(z)_i = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}} \quad (g(z)_i \in [0,1], \sum_{i=1}^k g(z)_i = 1)$$
- 特点：输出可解释为类别概率，但存在“赢者通吃”现象，对异常值敏感。

### 2.4 经典案例：LeNet网络
#### 2.4.1 结构流程
输入（32×32×1）→ C1（6个5×5卷积核，输出28×28×6）→ S2（2×2最大值池化，输出14×14×6）→ C3（16个5×5卷积核，输出10×10×16）→ S4（2×2最大值池化，输出5×5×16）→ C5（120个5×5卷积核，输出1×1×120）→ F6（全连接，84个神经元）→ Output（全连接，10个神经元，Softmax输出）。

#### 2.4.2 参数量计算
- C1：$(5×5×1+1)×6=156$
- C3：$(5×5×6+1)×16=2416$
- C5：$(5×5×16+1)×120=48120$
- F6：$(120×84)+84=10164$
- Output：$(84×10)+10=850$
- 总参数量：$156+2416+48120+10164+850=61706$。

---

## 三、CNN的训练方法
### 📌 核心逻辑：前向传播+反向传播（BP）+ 参数更新
### 3.1 通用数学模型
$$\underset{\theta}{argmin} O(\mathcal{D} ; \theta) = \sum_{i=1}^N \mathcal{L}(y_i, f(x_i;\theta)) + \Omega(\theta)$$
- $\theta$：待优化参数（卷积核权重、偏置）。
- $\mathcal{L}$：损失函数（如交叉熵、MSE）。
- $\Omega(\theta)$：正则化项（防止过拟合）。

### 3.2 卷积层的梯度计算与BP算法
#### 3.2.1 前向计算
$$Z^{l,p} = W^{l,p} \otimes X^{l-1} + b^{l,p}, \quad A^{l,p} = g(Z^{l,p})$$
- $W^{l,p}$：第l层第p个卷积核，$\otimes$为卷积运算，$g$为激活函数。

#### 3.2.2 梯度计算核心性质
- 损失对卷积核的梯度：$$\frac{\partial \mathcal{L}}{\partial W^{l,p,d}} = \delta^{l,p} \otimes X^{l-1,d}$$
  - $\delta^{l,p} = \frac{\partial \mathcal{L}}{\partial Z^{l,p}}$（第l层第p个特征图的误差项）。
- 损失对偏置的梯度：$$\frac{\partial \mathcal{L}}{\partial b^{l,p}} = \sum_{i,j} \delta_{i,j}^{l,p}$$（误差项求和）。
- 损失对输入的梯度：$$\frac{\partial \mathcal{L}}{\partial X^{l-1}} = rot180(W^{l,p}) \otimes \delta^{l,p}$$（卷积核旋转180°后与误差项卷积）。

#### 3.2.3 误差项跨层传播
| 下一层类型     | 误差传播公式                                  | 关键操作                                  |
|----------------|-------------------------------------------|-------------------------------------------|
| 池化层         | $\delta^{l,p} = up(\delta^{l+1,p}) \odot g'(Z^{l,p})$ | $up(\cdot)$：上采样（Max池化回传最大值位置，Average池化平均分配） |
| 卷积层         | $\delta^{l,d} = g'(Z^{l,d}) \odot \sum_{p=1}^{P} (rot(W^{l+1,p}) \otimes \delta^{l+1,p})$ | 卷积核旋转180°，与下一层误差项卷积后求和 |
| 全连接层       | $\delta^{l} = g'(Z^{l}) \odot (W^{l+1})^T \times \delta^{l+1}$ | 权重转置与下一层误差项矩阵乘法            |

### 3.3 训练技巧
#### 3.3.1 参数更新策略
$$W^{(t+1)} = W^{(t)} - \eta \cdot \frac{1}{M} \sum_{m=1}^M \frac{\partial \mathcal{L}_m}{\partial W}$$
$$b^{(t+1)} = b^{(t)} - \eta \cdot \frac{1}{M} \sum_{m=1}^M \frac{\partial \mathcal{L}_m}{\partial b}$$
- $\eta$：学习率，$M$为批量大小。

#### 3.3.2 关键训练技巧
1. **随机梯度下降（SGD）**：随机洗牌数据，用样本子集计算梯度，降低计算负担，易跳出局部极小值。
2. **带动量的SGD**：引入动量项$\alpha$，$$\Delta W = \alpha \cdot \Delta W_{prev} - \eta \cdot \nabla \mathcal{L}$$，缓解震荡，加速收敛。
3. **学习率衰减**：
   - 指数衰减：$\eta = \eta_0 e^{-kt}$。
   - 1/t衰减：$\eta = \eta_0 / (1+kt)$，避免后期震荡。
4. **数据增强**：
   - 几何变换：翻转、旋转、裁剪、缩放。
   - 灰度变换：亮度、对比度调整。
   - 混合增强：CutMix、Mixup（图像混合），提升泛化能力。
5. **批量归一化（BN）**：
   - 操作：$$\hat{z}^l = \frac{z^l - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \odot \gamma + \beta$$。
   - 优势：解决内部协变量偏移，加速训练，提高泛化能力，允许更大学习率。

---

## 四、CNN的发展历程
### 📌 核心趋势：更深、更高效、泛化能力更强
### 4.1 关键数据集：ImageNet
- 规模：约1419万张图像URL，覆盖21841个类别，支持细粒度分类。
- 里程碑：2012年AlexNet在ILSVRC挑战赛中Top-5错误率降至16%，开启深度学习视觉革命；2015年ResNet超越人类表现。

### 4.2 经典网络架构演进
| 网络名称       | 发布年份 | 核心创新                                  | 结构特点                                  | 性能（ILSVRC Top-5错误率） |
|----------------|----------|-------------------------------------------|-------------------------------------------|---------------------------|
| AlexNet        | 2012     | 首次用ReLU、Dropout、LRN、多GPU训练        | 8层（5卷积+3全连接），5×5卷积核          | 16.4%                     |
| VGGNet        | 2014     | 小卷积核（3×3）替代大卷积，更深网络（16/19层） | 分阶段卷积，特征共用，全连接层参数量大    | 7.3%                      |
| GoogLeNet（Inception V1） | 2014 | Inception模块（多尺度卷积融合），辅助分类器 | 22层，无全连接层，参数量小                | 6.7%                      |
| Inception V3   | 2016     | 多层小卷积（3×3替代5×5），BN，标签平滑    | 空间可分离卷积，假设空间更小，抗过拟合    | 3.5%                      |
| ResNet         | 2016     | 残差连接（跨层直连），缓解梯度消失        | 深度达152层，可训练超深层网络            | 3.6%                      |
| DenseNet       | 2017     | 密集连接（各层特征直接融合）              | 特征重用，参数量小，泛化能力强            | 2.8%                      |

### 4.3 核心创新点解析
#### 4.3.1 AlexNet
- ReLU激活：缓解梯度消失，稀疏激活（约50%神经元活跃）。
- LRN（局部响应归一化）：模拟生物侧抑制，增强泛化能力：
  $$Y^p = \frac{Y^p}{\left(k+\alpha \sum_{j=max(1,p-n/2)}^{min(P,p+n/2)} (Y^j)^2\right)^\beta}$$。
- Dropout：随机失活部分神经元，防止过拟合。

#### 4.3.2 VGGNet
- 小卷积核优势：3×3卷积核堆叠（2层=5×5感受野，3层=7×7），参数量更少（2×3²=18 < 5²=25）。
- 分阶段训练：先训练浅层模型，权重初始化深层模型，解决深层收敛困难。

#### 4.3.3 Inception V3
- 标签平滑：缓解one-hot标签的“过度自信”，$$y_k' = (1-\epsilon)y_k + \frac{\epsilon}{C-1}$$（$\epsilon$为平滑系数，C为类别数）。
- 批量归一化（BN）：稳定输入分布，加速训练，隐性正则化。

#### 4.3.4 ResNet
- 残差连接：$$H(x) = x + F(x)$$，F(x)为残差函数，更容易学习（残差接近0，初始化更简单）。
- 梯度传播高效：跨层直连使梯度直接回传浅层，缓解梯度消失，支持超深层网络。

---

## 五、CNN轻量化技术
### 📌 核心目标：在保证精度的前提下，降低参数量、计算量和存储开销，适配端侧设备
### 5.1 剪枝（Pruning）
#### 5.1.2 核心思想
移除网络中冗余的权重、神经元或通道，保留关键特征提取能力。
#### 5.1.2 分类与准则
| 剪枝类型       | 修剪单元                                  | 优势                                  | 劣势                                  |
|----------------|-------------------------------------------|---------------------------------------|---------------------------------------|
| 结构性剪枝     | 神经元/通道/卷积核                        | 轻量化效果明显，推理速度提升显著        | 精度损失可能较大                      |
| 非结构性剪枝   | 单个权重                                  | 精度损失小                              | 产生稀疏矩阵，需硬件支持才能提速        |

#### 5.1.3 剪枝准则
- 幅度准则：权重绝对值越小，贡献越小。
- Lₚ范数准则：卷积核权重范数越小，整体贡献越小。
- 梯度准则：权重梯度越小，对损失影响越小。
- 正则化引导：训练时引入L1正则化，迫使部分权重趋近于0。

### 5.2 量化编码（Quantization）
#### 5.2.1 核心思想
降低权重/激活值的比特位数（如32位浮点数→8位整数），减少存储和计算开销。
#### 5.2.2 优势
- 存储量：8位量化可减少75%存储（32→8bit）。
- 计算量：整数运算比浮点运算更快，功耗更低。
- 精度损失：合理量化（如Min-Max量化、KL散度校准）可保持精度基本不变。

#### 5.2.3 效果示例
| 网络名称       | 原始存储 | 量化后存储 | 原始精度 | 量化后精度 |
|----------------|----------|------------|----------|------------|
| LeNet-300      | 1720KB   | 44KB       | 98.36%   | 98.42%     |
| GoogleNet      | 28MB     | -          | 88.90%   | 88.92%     |

### 5.3 轻量化网络设计
#### 5.3.1 核心思路
从网络结构设计出发，通过高效卷积操作减少参数量和计算量。
#### 5.3.2 代表性技术
1. **深度可分离卷积（MobileNet）**：
   - 拆分：深度卷积（DW）+ 逐点卷积（PW）。
   - 计算量对比：传统卷积 = 9×(DW+PW)，参数量大幅降低。
   - 优势：适配移动设备，轻量化效果显著。

2. **点分组卷积（ShuffleNet）**：
   - 核心：1×1卷积分组计算，减少参数量（分组数g，参数量≈1/g²）。
   - 改进：通道重排（Channel Shuffle），解决分组卷积的通道隔离问题，保留特征融合能力。

3. **SqueezeNet**：
   - 策略：1×1卷积替代3×3卷积（参数量→1/9），减少3×3卷积通道数，降采样后置（保留更多特征）。
   - 核心模块：Fire模块（Squeeze层+Expand层）。

### 5.4 轻量化技术对比
| 技术类型       | 核心手段                                  | 参数量降低 | 计算量降低 | 精度保持 |
|----------------|-------------------------------------------|------------|------------|----------|
| 剪枝           | 移除冗余单元                              | 中-高      | 中-高      | 中       |
| 量化           | 降低比特位数                              | 高         | 高         | 高       |
| 深度可分离卷积 | 拆分卷积操作                              | 高         | 高         | 中-高    |
| 分组卷积       | 通道分组计算                              | 中-高      | 中-高      | 中-高    |

---

## 六、CNN综合应用
### 📌 典型场景
1. **计算机视觉任务**：图像分类、目标检测、语义分割、图像去噪/修复、超分辨率重建。
2. **跨领域应用**：雷达一维像目标检测（抗欺骗干扰）、医学图像分析、自动驾驶视觉感知。

### 📌 应用设计框架（以图像去噪为例）
1. 数据集制备：构建“干净图像-噪声图像”成对数据集，数据增强（翻转、旋转）。
2. 网络结构：U-Net（编码器+解码器），用卷积提取特征，反卷积恢复尺寸。
3. 损失函数：MSE（像素级误差）+ 感知损失（特征级误差）。
4. 训练策略：Adam优化器，学习率衰减，批量归一化。
5. 预测推理：输入噪声图像，网络输出去噪后的干净图像。

---

## 七、循环神经网络（RNN）课程回顾
### 📌 核心定位：处理时序数据的深层神经网络（后续章节详细展开）
### 7.1 基础回顾
- 与CNN的差异：CNN面向空间数据（图像），基于局部性和不变性假设；RNN面向时序数据（文本、语音），基于时序依赖假设。
- 核心思想：引入反馈连接，保留历史信息，处理序列数据的前后依赖关系。
- 前置知识：需掌握CNN的层次化表征思想，为RNN的时序特征提取奠定基础。

---

## 📋 核心总结
1. **CNN核心**：局部连接+参数共享，解决全连接网络的参数爆炸和空间信息丢失问题，实现层次化特征提取。
2. **关键组件**：卷积层（特征提取）、池化层（维度降低）、全连接层（分类回归），激活函数引入非线性。
3. **训练核心**：卷积层梯度计算基于卷积微分性质，误差项跨层传播需适配不同层类型，训练技巧（数据增强、BN、学习率衰减）提升泛化能力。
4. **发展趋势**：从深层（ResNet）到高效（Inception），再到轻量化（MobileNet、ShuffleNet），适配不同应用场景。
5. **轻量化方向**：剪枝、量化、高效卷积设计，平衡精度与资源开销。