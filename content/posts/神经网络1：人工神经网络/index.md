+++
date = '2025-11-29T17:02:54+08:00'
draft = false
title = '人工神经网络(ANN)'
tags = ["课程笔记", "神经网络"]
series = ["神经网络"]
series_order = 1
author = 'RayChaux'
math = true
+++

# 📚 神经网络方法与应用（2025）学习笔记
---

## 🧠 第一章 人工神经网络基础
核心目标：理解人工神经网络的定义、结构、数学模型及核心学习算法（LMS），掌握生物神经元与人工神经元的映射关系。

### 1.1 生物神经网络与人工神经网络的关联
- **生物神经网络**：
  - 基本单位：神经细胞（生物神经元），约120亿-140亿个高度互连
  - 核心结构：树突（信号输入）、轴突（信号输出）、突触（信号传递接口）
  - 关键特性：突触可塑性（学习记忆基础）、时空整合性、兴奋/抑制二元性
- **人工神经网络（ANN）**：
  - 定义（Kohonen 1988）：由具有适应性的简单单元组成的广泛并行互联网络，模拟生物神经系统对真实世界的反应
  - 定义（Haykin 1999）：由简单处理单元构成的大规模并行分布式处理器，天然具备存储经验知识并复用的特性
  - 核心特点：
    - 对生物神经网络的抽象、简化与模拟
    - 以神经元间联接权值存储知识
    - 通过学习从环境中获取知识
    - 大规模并行计算与分布式信息处理

### 1.2 基本人工神经元模型
#### 1.2.1 结构与数学描述
- 核心逻辑：多输入→线性组合→非线性激活→单输出
- 数学表达式：
  $$net_i = \sum_{j=1}^{n} w_{ij} \cdot x_j + b_i$$
  $$y_i = f(net_i)$$
  其中：
  - $x_j$：输入信号（对应树突接收的信号）
  - $w_{ij}$：连接权值（对应突触耦合强度）
  - $b_i$：偏置项（调整神经元激活阈值）
  - $net_i$：线性组合结果（对应细胞体整合信号）
  - $f(\cdot)$：激活函数（对应神经元非线性响应）
  - $y_i$：输出信号（对应轴突传出的信号）

#### 1.2.2 激活函数分类与特性
| 类型         | 常见函数                                                                 | 优势                                  | 劣势                                  |
|--------------|--------------------------------------------------------------------------|---------------------------------------|---------------------------------------|
| 单极性       | Sigmoid：$f(u)=\frac{1}{1+exp(-\alpha u)} \in [0,1]$                     | 平滑可微，输出映射到[0,1]             | 梯度消失、计算速度慢                  |
| 双极性       | 双曲正切：$f(u)=tanh(\gamma u)=\frac{1-e^{-2\gamma u}}{1+e^{-2\gamma u}}$ | 输出对称分布，梯度优于Sigmoid         | 仍存在梯度饱和问题                    |
| 分段线性     | ReLU：$f(u)=max(0,u)$                                                   | 防止梯度消失、计算速度快、非线性      | 神经元易"死亡"（输入≤0时梯度为0）     |
| 径向基       | 高斯RBF：$f(u)=\frac{1}{\sqrt{2\pi}\sigma}exp\left[-\frac{1}{2}\left(\frac{u-\mu}{\sigma}\right)^2\right]$ | 局部响应强，拟合非线性能力优          | 参数选择复杂                          |

> 💡 注意：深度神经网络中ReLU及其变种（如Mish）应用广泛，核心解决Sigmoid的梯度消失问题；胶囊神经网络是新型神经元模型，可保存姿态信息，实现"等变化"特性。

### 1.3 人工神经网络的结构类型
#### 1.3.1 按输入输出关系划分
- 静态神经网络：输出仅依赖当前输入，与历史输出无关（如MLP、RBF）
- 动态神经网络：输出依赖当前输入+历史输出，含反馈机制（如Hopfield网络）

#### 1.3.2 按连接结构划分
- 前馈式网络：信号单向传播，无反馈（输入层→隐含层→输出层）
- 反馈式网络：存在输出到输入的反馈连接（如Hopfield、LSTM）
- 其他结构：层内互连型、全互连型、局部互连型

#### 1.3.3 按学习方法划分
- 有监督学习：需标注训练数据（如分类、回归任务）
- 无监督学习：无需标注训练数据（如聚类任务）
- 强化学习：通过环境反馈（奖励/惩罚）学习

### 1.4 核心学习算法：LMS算法（Widrow-Hoff算法）
#### 1.4.1 算法基础
- 核心目标：通过迭代调整权值$w$，最小化均方误差（MSE）
- 自适应信号处理背景：修正系统特性以适应对象/扰动的动态变化，处理不确定性系统

#### 1.4.2 线性神经元LMS算法
- 误差定义：$\varepsilon_k = d_k - S_k = d_k - W_k^T \cdot X_k$（$d_k$为期望输出，$S_k$为线性组合输出）
- 均方误差：$MSE = E\{(d_k - W_k^T \cdot X_k)^2\}$
- 迭代公式：$w_{k+1} = w_k + 2\mu \varepsilon_k x_k$
  - $\mu$：学习率（步长），取值范围$0<\mu<\frac{1}{trace\{E(x_k x_k^T)\}}$
  - 核心逻辑：权值修正量与误差$\varepsilon_k$、输入$x_k$成正比，误差趋近于0时权值收敛

#### 1.4.3 非线性神经元LMS算法
- 误差定义：$\tilde{\varepsilon}_k = d_k - y_k = d_k - sgm(S_k)$（$sgm$为激活函数）
- 迭代公式：$w_{k+1} = w_k + 2\mu \tilde{\varepsilon}_k \cdot (1-y_k^2) X_k$
- 关键差异：比线性LMS多激活函数导数项$sgm'(S_k)=1-y_k^2$（双曲正切激活时）

#### 1.4.4 应用场景：红外弱小目标检测
- 问题：低SCR图像中目标被杂波/噪声淹没
- 解决方案：自适应LMS滤波预白化噪声，提高信杂比
- 滤波公式：$y(m,n)=W^T X_{mn}$，误差$e(m,n)=d(m,n)-y(m,n)$
- 优势：运算量小，可实时实现

> 📝 核心结论：神经网络的核心是权值学习，LMS算法是基础自适应学习方法，线性与非线性版本的核心区别在于是否包含激活函数导数项。

---

## 📊 第二章 机器学习基本知识
核心目标：掌握机器学习的定义、三要素、流程、评估方法及数学基础，理解AI/ML/DL的层级关系。

### 2.1 核心概念体系
#### 2.1.1 AI、ML、DL的层级关系（套娃关系）
- 人工智能（AI）：模拟、延伸和扩展人类智能的理论、方法与系统
  - 关键定义（斯坦福）：研究使计算机执行需人类智能完成的任务的科学与工程
  - 核心能力：演绎推理、知识表示、学习、运动控制、数据挖掘等
- 机器学习（ML）：AI的子集，机器自动从经验中学习并优化，无需人工编程规则
- 深度学习（DL）：ML的子集，使用深层神经网络（DNN）进行特征提取与建模

#### 2.1.2 机器学习的定义
1. Mitchell（1997）定义：若程序通过经验$E$在任务$T$上的性能$P$得到改善，则称程序对$E$学习
2. 周志华（2016）定义：通过计算手段利用经验（数据）改善系统性能，从数据中产生模型用于决策
3. 通俗定义：通过算法从大量数据中自动学习规律，对新样本做决策（本质是"参数估计"）

#### 2.1.3 机器学习三要素
| 要素               | 核心作用                                                                 |
|--------------------|--------------------------------------------------------------------------|
| 模型假设           | 划定$Y$与$X$的关系空间（如线性假设$Y=WX+b$），避免盲目搜索               |
| 评价函数（损失）   | 定义"最优"标准（如最小化拟合误差），常用损失函数：均方误差、交叉熵等     |
| 优化算法           | 寻找使评价函数最优的参数（如梯度下降法），核心是高效寻解                 |

### 2.2 机器学习的核心流程
1. **训练阶段（归纳）**：从具体样本（$X,Y$）中提炼$Y$与$X$的关联规律
2. **预测阶段（演绎）**：基于训练得到的模型，对新输入$X$计算输出$Y$，验证模型有效性

### 2.3 关键术语
#### 2.3.1 数据相关
- 训练集：用于模型学习的标注数据（如"好瓜/坏瓜"样本）
- 测试集：用于验证模型泛化能力的独立数据（与训练集互斥）
- 特征：数据的属性（如瓜的色泽、根蒂、敲声）
- 标记：数据的类别/数值（如"好瓜"为分类标记，成熟度为回归标记）
- 独立同分布（i.i.d）：样本服从未知分布$D$，独立采样获得

#### 2.3.2 任务类型
- 分类任务：预测离散值（二分类：好瓜/坏瓜；多分类：冬瓜/南瓜/西瓜）
- 回归任务：预测连续值（如瓜的成熟度、房价）
- 聚类任务：无标记数据分组（如自动划分瓜的品种）

#### 2.3.3 学习类型
- 监督学习：含标记信息（分类、回归），算法示例：朴素贝叶斯、SVM、决策树、逻辑回归
- 无监督学习：无标记信息（聚类），算法示例：K-means、DBSCAN、PCA、LDA
- 半监督学习/弱监督学习：部分标记数据
- 强化学习：智能体与环境交互，通过奖励信号优化策略（"有延迟的监督学习"）

#### 2.3.4 泛化能力
- 定义：模型适应新样本的能力（核心目标）
- 影响因素：训练样本数量（越多越易获得强泛化能力）、模型复杂度、数据分布一致性

### 2.4 优化算法
#### 2.4.1 梯度下降法基础
- 梯度定义：$\nabla f(x,y)=\left(\frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y}\right)$
- 迭代公式：$\vec{x}_{n+1}=\vec{x}_n - \lambda \nabla f(\vec{x}_n)$（$\lambda$为学习率）
- 方向导数：$\frac{\partial f(x,y)}{\partial \vec{e}} = \vec{e} \cdot \nabla f(x,y)$，梯度方向是函数增长最快方向

#### 2.4.2 梯度下降法变种对比
| 类型                | 核心逻辑                          | 优势                                  | 劣势                                  |
|---------------------|-----------------------------------|---------------------------------------|---------------------------------------|
| 批量梯度下降（BGD） | 全量样本计算梯度更新一次          | 收敛稳定，全局最优                    | 样本量大时迭代速度慢                  |
| 随机梯度下降（SGD） | 单个样本计算梯度更新一次          | 迭代速度快，可跳出局部最优            | 无向量化加速，收敛波动大              |
| 小批量梯度下降（MBGD） | 部分样本（mini-batch）计算梯度更新 | 平衡速度与稳定性，工程常用            | 需调优batch size，可能小幅波动        |

> 💡 注意：可通过学习率衰减缓解SGD和MBGD的收敛波动问题。

### 2.5 模型评估与选择
#### 2.5.1 误差类型
- 错误率：$E = a/m$（$a$为错分样本数，$m$为总样本数）
- 训练误差（经验误差）：模型在训练集上的误差
- 测试误差：模型在测试集上的误差（泛化误差近似）
- 泛化误差：模型在所有潜在样本上的误差

#### 2.5.2 过拟合与欠拟合
| 问题       | 表现特征                                  | 解决方案                                  |
|------------|-------------------------------------------|-------------------------------------------|
| 过拟合     | 训练误差小，泛化误差大（学透训练集噪声）  | 正则化、早停（early stop）、增加训练数据   |
| 欠拟合     | 训练误差大，泛化误差大（未学好核心规律）  | 增加模型复杂度（决策树拓展分支、神经网络增加轮数） |

#### 2.5.3 评估方法
1. **留出法**：
   - 划分训练集$S$与测试集$T$（互斥），保持数据分布一致
   - 比例：训练集:测试集=2:1~4:1，多次随机划分取平均
2. **交叉验证法**：
   - 分层采样划分为$k$个互斥子集，$k-1$个为训练集，1个为测试集
   - 常用$k=10$（10折交叉验证），返回$k$次测试结果均值
3. **自助法**：
   - 对数据集$D$采样$m$次得到训练集$D'$，未采样样本（约1/3）为测试集
   - 适用场景：数据集较小，避免训练/测试集划分困难；缺点是改变数据分布

#### 2.5.4 性能度量
##### 回归任务
- 均方误差（MSE）：$E(f;D)=\frac{1}{m}\sum_{i=1}^m (f(x_i)-y_i)^2$

##### 分类任务
1. 基础度量：
   - 错误率：$E(f;D)=\frac{1}{m}\sum_{i=1}^m I(f(x_i)\neq y_i)$
   - 精度：$acc(f;D)=1-E(f;D)=\frac{1}{m}\sum_{i=1}^m I(f(x_i)=y_i)$
2. 混淆矩阵与查准率/查全率：
   | 真实情况\预测结果 | 正例（TP） | 反例（FN） |
   |--------------------|------------|------------|
   | 正例               | 真正例     | 假反例     |
   | 反例               | 假正例（FP） | 真反例（TN） |
   - 查全率（召回率）：$R=\frac{TP}{TP+FN}$
   - 查准率（精确率）：$P=\frac{TP}{TP+FP}$
3. P-R曲线与F1度量：
   - P-R曲线：按正例可能性排序样本，逐个预测得到的查准率-查全率曲线
   - F1分数（调和平均）：$F1=\frac{2×P×R}{P+R}=\frac{2×TP}{样本总数+TP-TN}$
   - 加权F1（$F_\beta$）：$\beta>1$偏重查全率，$\beta<1$偏重查准率
4. ROC曲线与AUC：
   - ROC曲线：横轴（假正例率$Pf=\frac{FP}{TN+FP}$），纵轴（真正例率$Pd=\frac{TP}{TP+FN}$）
   - AUC计算：$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\cdot(y_i+y_{i+1})$，衡量排序质量
5. 代价敏感错误率：
   - 适用场景：不同错误类型代价不同（如医疗诊断）
   - 代价曲线：反映不同条件下的期望总体代价

#### 2.5.5 偏差-方差分解
- 泛化误差公式：$E(f;D)=bias^2(x)+var(x)+\varepsilon^2$
  - 偏差：$bias^2(x)=(\bar{f}(x)-y)^2$（算法期望预测与真实结果的偏离）
  - 方差：$var(x)=E_D[(f(x;D)-\bar{f}(x))^2]$（训练集变动导致的性能波动）
  - 噪声：$\varepsilon^2=E_D[(y_D-y)^2]$（任务本身的难度下界）
- 偏差-方差窘境：
  - 训练不足时：偏差主导泛化误差（模型拟合能力弱）
  - 训练充足时：方差主导泛化误差（模型过拟合训练集噪声）

### 2.6 机器学习常用定理
1. **PAC学习理论（Probably Approximately Correct）**：
   - 核心：算法能在多项式时间内，从合理数量训练数据中学习到近似正确的$f(x)$
   - 极限性质：$lim_{|D|\to\infty} \mathcal{R}(f)-\mathcal{R}_D^{emp}(f)=0$（经验风险趋近期望风险）
   - 概率保证：$P((\mathcal{R}(f)-\mathcal{R}_D^{emp}(f))\leq\epsilon)\geq1-\delta$（$0<\epsilon,\delta<0.5$）

2. **奥卡姆剃刀原理**：
   - 核心：如无必要，勿增实体（简单模型泛化能力更优）
   - 应用：引入正则化限制模型复杂度，避免过拟合

3. **没有免费午餐定理（NFL）**：
   - 核心：不存在对所有问题都有效的优化算法，某类问题有效则另一类问题可能劣于随机搜索

4. **丑小鸭定理**：
   - 核心：世间无客观相似性准则，一切标准皆主观（如丑小鸭与白天鹅的相似性取决于评判维度）

### 2.7 机器学习的数学基础
| 数学分支     | 核心内容                                                                 |
|--------------|--------------------------------------------------------------------------|
| 线性代数     | 向量、矩阵、张量（高维推广）、线性变换、特征值/特征向量、矩阵分解         |
| 微积分       | 导数、微分、泰勒公式、积分、矩阵微积分（向量函数导数）                   |
| 数学优化     | 无约束/约束优化、全局/局部最小解、梯度下降法、拉格朗日乘数法、KKT条件   |
| 概率论       | 随机变量、条件概率、贝叶斯定理、期望/方差、马尔可夫过程、高斯过程         |
| 信息论       | 信息熵（$H(x)=-\sum_{i=1}^n P_i ln P_i$）、互信息、交叉熵、KL散度、JS散度 |

> 📝 核心结论：数学是机器学习的基础，线性代数处理数据表示，微积分支撑优化算法，概率论建模不确定性，信息论衡量数据相关性。

---

## 📋 总结
1. 人工神经网络：模拟生物神经元的并行计算系统，核心是权值学习（LMS算法为基础），结构分为静态/动态、前馈/反馈等类型。
2. 机器学习：AI的核心子集，三要素（模型假设、评价函数、优化算法）驱动，通过训练/预测流程实现从数据到模型的转化，泛化能力是核心目标。
3. 层级关系：AI ⊇ ML ⊇ DL（深度学习=深层神经网络+机器学习），数学基础（线代、微积分、概率等）是理论支撑。
4. 实践关键：平衡模型复杂度（缓解偏差-方差窘境）、选择合适的评估方法与性能度量、合理设计学习算法（如梯度下降变种）。